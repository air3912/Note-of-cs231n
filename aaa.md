

### 神经网络出现前的史前时代（线性/仿射）

这时候都是些基础的算法实现，是大模型的基础，但是做出来的模型不是很理想,但是有些算法依旧是现代模型中重要的组成部分

#### KNN
k-nearest neighbors

目的：图像分类（输入的图片与训练集一一对照）


超参数：k，距离计算方式


流程：
当KNN接收到一张新的、未知的图片时，它会做以下三步工作：

1. 比对： 把这张新图片，跟训练集里每一张老照片进行对比，计算它们之间的相似度。
2. 筛选： 找出距离最近的K张老照片。
3. 投票： 看这3张老照片是什么标签。如果其中2张是狗，1张是狼，那么KNN就会宣布这张新照片是狗

计算方式：图片转化为数据，然后计算相同位置上的距离（加根号不加根号两种方式）



碎碎念：方法很笨，而且计算量有些吓人

#### SVM
support vector machine

目的：图像分类（在训练之间划边界线，看输入的数据在哪边）

超参数：
- 安全距离：一般设置为1，看正确答案得分比错误答案高出多少，如果大于这个1就不用管了，否者损失增加（错误答案得分+1-正确答案的得分）答案不仅要赢，而且要赢得漂亮
- 学习率：指一次调整权重矩阵的距离

流程: 

1. 训练：随机设置参数矩阵W的数值，染后计算每个答案的得分，参照标准答案计算得到损失，对于计算得到loss的部分数据，我们计算梯度反向转播然后根据设置学习率修改导致问题的矩阵W数据，最后得到的就是（注意偏置b，单独计算调整数值有些过于麻烦，所以我们一般就把权重矩阵多加一列随机数，然后输入向量x也对应的加一行1就可以了

2. 分类：有了W，直接用公式$f(x) = Wx + b$可以计算得分

损失函数：正则化损失加合叶损失（就是之前的分数差）

碎碎念：开始引入参数权重矩阵W，是神经网络的祖宗

#### softmax 
直观上看，softmax和上面svm只有一点不同：
svm输出的是得分，而softmax会根据得分输出概率，最后根据概率不断优化

损失函数：正则化损失加交叉熵损失：$$L_i = -\log(P_{y_i})$$

#### 正则化与鲁棒性
例如$x=[1,1,1,1]$

没有正则化：模型可能把权重全压在第一个像素上。如果那个像素被污染了，模型就完了

有正则化：模型被迫把权重分散到4个像素上。这样即使坏了一个像素，其他三个还能撑住，结果不会差太多

碎碎念：说白了，防止过拟合就是防止一个权重矩阵的某个参数储存信息过多，这也就意味着，一个数值越大，他存储的信息越多？但是如果从embedding的角度思考，一个数值应该只能影响一部分指标的程度才对啊，而且这部分应该是确定的，为什么可以一个数储存多维度信息呢

### 神经网络诞生
 
#### 激活函数

激活函数是神经网络非线性的关键，不加激活函数你从左向右一次乘100000000个W，依旧是线性，这个很好理解，但是激活函数的种类是需要说道说道的

1. sigmoid：$$\sigma(x) = \frac{1}{1+e^{-x}}$$

2. Tanh: $$f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

3. Relu: $$f(x) = \max(0, x)$$

4. Leaky Relu：$$f(x) = \max(0.01x, x)$$

然后就是sigmoid和tanh在RNN里有，第三种在很多地方用的比较多

#### 全连接网络（MLP）
fully connected network

输入：会把图片拉成长条状，比如32*32*3会先处理成3072个一维的向量

隐藏层:
一般就是n层线性分类，然后激活函数

输出：各个分类的分数

定位：直接处理图片数据不太现实，一般是处理卷积提取的特征数据，很少作为backbone的部分，基本都是出现在最后的head

#### 卷积神经网络（CNN）

卷积核(The Filter/Kernel)：深度必须与输入深度保持一致，然后输出深度取决于卷积核数量

计算：直接点积所有元素然后相加,外围不够大就补0，然后设置步长，或者设池化层

定位：一般需要很多层，然后下面是AI的解释，我感觉很好，浅层的卷积核看线条，深层的卷积核看形状

---

神经网络是分很多层的（比如 VGG 有 13 层卷积）。每一层都在处理上一层的输出。

第 1 层（浅层）：它是“近视眼”。

它只看得到极其微小的局部。

它看到的是： “这里有一条横线”、“这里有一个黑点”、“这里是红色的”。（这就是最基础的积木块）。

第 5 层（中层）：它是“组装工”。

它把第 1 层找到的线条拼起来。

它看到的是： “两条横线 + 两条竖线 = 一个圆圈（可能是眼睛）”、“两条斜线 = 一个尖角（可能是耳朵）”。

第 10 层（深层）：它是“鉴定师”。

它把第 5 层的部件再拼起来。

它看到的是： “圆圈（眼睛） + 尖角（耳朵） + 毛茸茸的纹理 = 猫的脸”。

本质： CNN 不是一步到位认出猫的。它是一层一层地把简单的线条，组合成复杂的形状，再组合成完整的物体。

---

#### 循环神经网络（RNN）
recurrent neural network

解释一下：$h_t = \tanh( W \cdot [h_{t-1}, x_t] + b )$等价于$h_t = \tanh( \underbrace{W_h \cdot h_{t-1}} + \underbrace{W_x \cdot x_t} + b )$



定位：拥有记忆的神经网络，可以关注到上文关系（感觉是自然语言处理那边的重点）

特点：他会一个token一个token的处理,前面的输入会影响到后面的输出，就是会对前面的留有记忆。
时间步，表示每一个循环，包含一次输入，一次输出，一次隐藏状态的更新。h表示隐藏状态，用于储存记忆以及输出，


输入：一句话

处理：
- 普通RNN：只引入一个隐藏状态h，每个时间步会进行一下处理$$h_t = \tanh( W \cdot [h_{t-1}, x_t] + b )$$然后每一层的隐藏状态通过全连接层得到输出

- LSTM （long short-term memory）：普通RNN只有短期记忆，LSTM解决了这一限制。多引入了一个cell state（$c_t$）和三个门，激活函数是sigmoid，公式如下：
$$a_f = W_f \cdot [h_{t-1}, x_t] + b_f$$
$$a_i = W_i \cdot [h_{t-1}, x_t] + b_i$$
$$a_g = W_g \cdot [h_{t-1}, x_t] + b_g$$

整体是有两条线的，一个隐藏状态，一个细胞状态。

1. 遗忘门$f$，作用于细胞状态，取决于输入与上一层隐藏状态（显然，什么该遗忘一定程度上是取决于你这次的输入的，比如你换个主语）

2. 输入门$o$，作用于细胞状态，同样取决于x和h

3. 输出们$o$，作用于本时间步中修改过的细胞状态（作用前海选哟tanh激活一下），然后可以得到下一层的隐藏状态，同时本层的隐藏状态经过一层普通映射可以得到本时间步的输出


### 几个经典backbone

#### Alexnet
很少用了，但是说一下当时的几个创举：
1. 首次使出Relu激活函数
2. 首次采用dropout随机失活部分神经元，防止过拟合
3. 重叠池化，就是3*3的核但是步长是2。
4. 数据增强，防止过拟合




#### VGG

似乎没有什么可以说道的地方，就是结构很整齐，把网络结构给标准化了，告诉了人们比起大卷积核不如去搞深度，但是太深了也不行，网络会退化，学不动了说是

现在小数据样本多用这个


#### ResNet（residual networks）

首次提出跳跃链接解决网络层数太深导致的问题，具体做法如下：

- 相同维度：直接每个元素相加
- 不同维度：
1. 深度不同：把原图进行1*1卷积操作，数量取决于想要的深度（有个补0的方法，但是效果似乎不是很好）
2. H和W不同：同样通过卷积或者池化化成相同的就可以了

#### GoogleNet

它主要是由一个个inception组成的，然后这里介绍一下inception的基础结构就完事了（当然在处理之前会进行池化降低分辨率和减少计算量）

inception会同时走4条路线：
线路1： $1 \times 1$ 卷积。
线路2： $1 \times 1$ 卷积 $\rightarrow$ $3 \times 3$ 卷积。
线路3： $1 \times 1$ 卷积 $\rightarrow$ $5 \times 5$ 卷积。
线路4： $3 \times 3$ 最大池化 $\rightarrow$ $1 \times 1$ 卷积。

然后通过填充确保所有结果的H和W维度相同的情况下在深度上进行叠加就是答案了


然后他的输出分类头也有些特殊的地方（在当时是创举）就是他会先进行全局平均池化GAP（global average pooling）：

输入： $7 \times 7 \times 1024$
GAP操作： 对 1024 个 $7 \times 7$ 的面分别求平均。
输出： $1 \times 1 \times 1024$ 


### transformer

#### 注意力机制
我们有着统一的注意力公式：
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

##### 自注意力
编码器的核心，QKV全部直接来自输入的句子

##### 交叉注意力
用在解码器，编码器的输出经过两个矩阵投影得到K和V,然后Q是前面输出所有token的向量

##### 掩码注意力
主要在训练的时候用吧，然后就是防止模型作弊具体实现就是在给$QK^T$加上一个mask矩阵，矩阵左下角以及对角线都是0，右上角是负无穷，如此便有效隐藏了语句中前者对后者的注意力得分

##### 多头注意力
把一个token的embedding的维数平均分成几份，不过这个不是直接切开分的，而是通过几个不同的矩阵分的，比如输入维度是8*8，分成4份，就是乘4个不同的8*2矩阵，然后把得到的结果拼起来，就可以做到多头注意了


#### 位置编码
每一个token作为embedding输入时要加一个位置编码，正常语句的位置编码公式：

对于向量中的 偶数索引 (Even indices) 位置 $2i$：$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$对于向量中的 奇数索引 (Odd indices) 位置 $2i+1$：$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$


#### Add and Norm
残差连接与归一化，不同于BN，这个是将每一个token的embedding归一化


#### 前馈神经网络（FFN）
三明治结构，两个线性变化中间夹个激活函数用于思考通过attention得到的数据







### 自监督学习(SSL)
以前的训练学习都是....怎么说呢，指定性过于明确了，自监督其实就是学习的更广泛了，他学习的直接是事物的特征，但是计算损失反向转播调整参数的操作依旧，知识学习的方向不一样

与传统的训练其实没有差多少我觉得，至少架构是一样的，但是自监督看的更远这点无疑



#### 早期探索
早期训练主要有三种方式：拼图，上下文预测（类似完形填空），旋转预测

#### 对比学习
核心在于正负样本，我们输入很多图像，然后机器需要让正样本在特征空间尽可能接近

#### 跨模态自监督
例如CLIP，这是靠openai从网上爬出的带文字描述的图片训出来的，结合视觉和语言、

训练过程中我们同时将文字和图片输入不同的编码器得到特征向量，监督信号是只有原图和描述的向量点积最大才行，其他组合应该不相似


#### 深度强化学习(Deep Reinforcement Learning, DRL)
感觉还是改变的只是训练的方向，训练的那些基础的流程什么的都一样，然后具体实现算法也没有涉及



### 计算机视觉

#### Vit
就是transformer在视觉领域的运用，以往transformer的注意力机制和位置编码都明显感觉到这个是偏向于NLP方向的一个模型，但是Vit将注意力机制引入了我们视觉领域，

处理：将图片分成一个个patches，假设是16*16*3的像素块，我们直接把他展平然后输入一个全连接层然后得到的输出作为embedding，然后再加上我们Vit中独特的位置编码公式：
$$z_0 = [x_{class}; x^1_p E; x^2_p E; ...; x^N_p E] + E_{pos}$$
但是注意，我们的Vit并没有解码器结构，使用的是CLS token，就是在输入前再加一个随机token，然后在一次次自注意力中，这个token会染上所有patches的信息，最终得到的可以代表全图的输出就是这个token的输出向量，最后连接全连接层完成分类。

当然，也可以取所有patches的输出取平均值然后得到特征向量作为分类头的输入





#### 目标分割与检测

##### FCN
可以实现像素级别的分割（热力图那种）
不再有全连接层，全部换成卷积层，最后输出的深度是需要判断的元素种类的概率分布
假如我们一共有20个可能的待检测目标，如果只是判断整体，最后输出一个1*1*20的很简单，但是想做到像素级别的分类，需要输出维度的W和H与输入相同，这就需要我们记录每一次卷积降维后得到的特征图，然后配合深层特征图进行上采样，其实就是直接反卷积（反卷积就是加0扩充维度，然后再进行卷积操作，图就放大了）然后通过跳跃连接和上采样，维度方面就不是问题了，输出n*n*20的结果轻轻松松



##### 单阶段探测器
速度块，精度低（不是特别低）
主要就是yolo （you only look once）把回归和分类结合在一起（v1版本）


操作：
1. 把一张图分成7*7块，然后设置B，就是一个方格最多检测的物品总数，这里B=2
2. 方格会判断这个东西中心的x,y（中心相对于网格左上角的偏移量）,w,h（预测网格的宽和高）以及置信度c（没有中心直接置0，否则再计算IoU，看框的怎么样，得到的就是置信度）
3. 假如一共有20个类别，输出的格式就会是$7 \times 7 \times (2 \times 5 + 20) = 7 \times 7 \times 30$，

缺陷：从这个输出格式就可以看出来问题了，如果一个框里有两个不同的物体，他输出的概率分布是一样的。。。这样就出问题了，所以对于东西又小又多的时候是会出问题的，但是后面的版本改进了这些问题




##### 双阶段探测器
就是R-CNN系列
精度高，速度慢（不是特别慢）
- R-CNN
根据所有训练的 SVM 指定的物品，找出并框选出图片中的对应的物品，剩下的全部当作背景
1. Selective Search 算法筛选候选框
2. 将每一个候选框都统一缩放扭曲成 227*227 的正方形
3. 训练出一推 SVM，然后依次作用，最后取分最高的一位作为判断结果
4. 再训练一个回归器，微调框的位置和大小
最原始的一代，缺点很多，最明显的说就是明显过大且冗余的计算量

- fast R-CNN
fast 版本就是把原本的的先框选再分别卷积变成了先真题卷积得到整体特征图再找到每个框选图
在特征图中对应的位置
其中的核心升级是:ROI Pooling 以及改 SVM 为 softmax classifier，还有回归损失
ROI Pooling：普通 Pooling：模具大小是死的，产出的结果大小随缘（看输入多大）
RoI Pooling：产出的结果大小是死的，模具大小随缘（自动调整以适应输入）
回归损失与分类损失结合：

- faster R-CNN
这个版本主要优化的是找框问题，新加入的升级就是 RPN：每个像素点整 9 个 anchors，然后 rpn会依次遍历每一个 anchor 如果只是背景就直接扔掉，但是如果是有东西就会被筛选出来，然后由 NMS（非极大抑制值，通过IoU计算来删除重复的框）来定义任何两个框是狗重合度过高，过分重合的就进行删除，只保留得分最高的


##### DETR
整体流程如下：
1. 先用 ResNet 把图片卷一遍，提取特征（就像ViT的patch embedding一样，把图变成特征序列）
2. Encoder，这里的 Encoder 和 ViT 差不多，利用Self-Attention 让特征之间互相交流，理解全局图像上下文

3. Decoder，上面的基本和Vit同理，没什么好讲的，这个decoder是detr的关键点，核心概念：object queries，一组参数，或者说一个100维随机初始化的向量，这个向量会和编码器输出的KV做交叉注意力计算，然后对于重复的，就是如果图中的物体数量小于100，不会用NMS，而是使用一个叫匈牙利算法的东西，可以做到只保留预测最好的正确的框


4. 最后在接一个简单的全连接层，输出框坐标 $(x, y, w, h)$ 和类别。

#### 视频理解

##### 3DCNN
我们认为图片是3维的，那么就可以把视频作为加上时间这一维度的四维，就是卷积核在k\*k 之外再加一项变成 k*k\*d（d是时间深度） 就是所谓的3D，它可以在时间维度上提取帧与帧之间的关系



##### 双流网络

对于一个视频我们同时看的有两点，其一是空间流，其二是时间流，也就是光流，光流描述了像素点运动的情况，就是通过3D CNN进行提取的


#### 生成模型

现在用的比较多的是扩散模型，生成对抗网络和变分自编码器我就只作了了解


##### 自回归模型
一个像素一个像素的生成，且后面的像素十分依赖前面的像素$$P(x) = \prod_{i=1}^{n} P(x_i | x_1, ..., x_{i-1})$$
说实话我感觉很奇怪，因为只靠这个逐个像素的生成有点太离谱了，数据量要求感觉会非常大，不过毕竟是早期的探索结果现在也没什么人用了


##### 变分自编码器

我们希望把图片压缩成一个简短的“编码”（Latent Vector $z$），比如“戴眼镜的男人”。然后从这个编码中还原出图片。
架构：Encoder： 把图片 $x$ 压缩成分布 $q(z|x)$（通常是高斯分布）。
Sampler： 从分布中采样一个 $z$。
Decoder： 把 $z$ 还原成图片 $\hat{x}$。

##### 生成对抗网络（GAN）
博弈这一块，互相进步学习



问题：A. 模式坍塌 (Mode Collapse)
现象： G 发现了一种特定的图片能骗过 D（比如这种图片里全是黄色的），它就开始只生成这一张图。结果： 无论你输入什么随机噪声 $z$，输出的都是同一张黄色的图。多样性完全丢失。
B. 训练不稳定性 (Instability)
现象： Loss 曲线不像 ResNet 那样平滑下降，而是像过山车一样震荡，甚至无法收敛。
原因： 寻找纳什均衡是一个动态平衡过程，很难达到。一旦 D 太强，G 就没有梯度可学了；一旦 G 太强，D 就乱猜了。需要非常小心地调节两者的学习率。
##### 扩散模型

核心思想就是去噪声，一张纯噪声的图，通过训练如何去噪声一步一步去噪便可以得到一个清晰的图片


训练:神经网络会接受一个被处理过的图（有噪声）以及当前图的时间步（处理次数）
神经网络需要回答的是上一个时间步是什么样的，即当前时间步所加上的噪声长什么样


预测噪声用的是U-net，核心思想有FCN里上采样加跳跃连接的影子，但是FCN的连接倾向于加法，但是U-net的连接是直接在深度上拼接



